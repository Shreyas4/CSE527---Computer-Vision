{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Paratkar_Shreyash_112673930_hw5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVm3kTY4lCtq",
        "colab_type": "code",
        "outputId": "93ebde15-6331-4460-bebb-f20966bba1ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvoYjmx0EwHm",
        "colab_type": "code",
        "outputId": "ea9bf504-a2e4-4f17-e739-72ee74dc267a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFmuqcF4lMtV",
        "colab_type": "code",
        "outputId": "160bf530-39bf-49c1-ec38-ef20b5450275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/gdrive/My Drive/Y2019Fall/CSE-527-Intro-To-Computer-Vision/Paratkar_Shreyash_112673930_hw5'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Y2019Fall/CSE-527-Intro-To-Computer-Vision/Paratkar_Shreyash_112673930_hw5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWODDNdRlSrs",
        "colab_type": "code",
        "outputId": "1d0ca45e-6114-466b-cc3d-636e44578c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%pwd\n",
        "%ls -lrt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8455342\n",
            "-rw------- 1 root root 8658247680 Oct 18 17:35 UCF101_images.tar\n",
            "drwx------ 2 root root       4096 Nov 20 11:13 \u001b[0m\u001b[01;34mannos\u001b[0m/\n",
            "-rw------- 1 root root      17664 Nov 21 03:49 CSE527_HW5_fall19.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtB_8Ah2ld1s",
        "colab_type": "code",
        "outputId": "912d48dc-970a-4f3e-a65b-e2fa113ee8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget 'http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_images.tar'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-21 03:39:18--  http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_images.tar\n",
            "Resolving vision.cs.stonybrook.edu (vision.cs.stonybrook.edu)... 130.245.4.232\n",
            "Connecting to vision.cs.stonybrook.edu (vision.cs.stonybrook.edu)|130.245.4.232|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8658247680 (8.1G) [application/x-tar]\n",
            "Saving to: ‘UCF101_images.tar.1’\n",
            "\n",
            "UCF101_images.tar.1 100%[===================>]   8.06G  31.9MB/s    in 4m 30s  \n",
            "\n",
            "2019-11-21 03:43:48 (30.6 MB/s) - ‘UCF101_images.tar.1’ saved [8658247680/8658247680]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y60IW9hsznJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xkf './UCF101_images.tar' 2>/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAHQi5ZpOSFt",
        "colab_type": "code",
        "outputId": "2312043d-386f-4735-a14f-1c7f568d2209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/gdrive/My Drive/Y2019Fall/CSE-527-Intro-To-Computer-Vision/Paratkar_Shreyash_112673930_hw5/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Y2019Fall/CSE-527-Intro-To-Computer-Vision/Paratkar_Shreyash_112673930_hw5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axgnLnEuy4CE",
        "colab_type": "code",
        "outputId": "8214226c-508e-4f0b-f455-8130b1e2cbb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%ls -lrt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8455446\n",
            "drwx------ 13322 root root       4096 May 31  2017 \u001b[0m\u001b[01;34mimages\u001b[0m/\n",
            "-rw-------     1 root root 8658247680 Oct 18 17:35 UCF101_images.tar\n",
            "drwx------     2 root root       4096 Nov 20 11:13 \u001b[01;34mannos\u001b[0m/\n",
            "-rw-------     1 root root      66439 Nov 21 08:20 train_video_label_df.pkl\n",
            "-rw-------     1 root root      26417 Nov 21 08:20 test_video_label_df.pkl\n",
            "-rw-------     1 root root      26921 Nov 21 08:31 Paratkar_Shreyash_112673930_hw5.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AcamCuuyjYb2"
      },
      "source": [
        "# Action Recognition @ UCF101  \n",
        "**Due date: 11:59 pm on Nov. 19, 2019 (Tuesday)**\n",
        "\n",
        "## Description\n",
        "---\n",
        "In this homework, you will be doing action recognition using Recurrent Neural Network (RNN), (Long-Short Term Memory) LSTM in particular. You will be given a dataset called UCF101, which consists of 101 different actions/classes and for each action, there will be 145 samples. We tagged each sample into either training or testing. Each sample is supposed to be a short video, but we sampled 25 frames from each videos to reduce the amount of data. Consequently, a training sample is an image tuple that forms a 3D volume with one dimension encoding *temporal correlation* between frames and a label indicating what action it is.\n",
        "\n",
        "To tackle this problem, we aim to build a neural network that can not only capture spatial information of each frame but also temporal information between frames. Fortunately, you don't have to do this on your own. RNN — a type of neural network designed to deal with time-series data — is right here for you to use. In particular, you will be using LSTM for this task.\n",
        "\n",
        "Instead of training an end-to-end neural network from scratch whose computation is prohibitively expensive, we divide this into two steps: feature extraction and modelling. Below are the things you need to implement for this homework:\n",
        "- **{35 pts} Feature extraction**. Use any of the [pre-trained models](https://pytorch.org/docs/stable/torchvision/models.html) to extract features from each frame. Specifically, we recommend not to use the activations of the last layer as the features tend to be task specific towards the end of the network. \n",
        "    **hints**: \n",
        "    - A good starting point would be to use a pre-trained VGG16 network, we suggest first fully connected layer `torchvision.models.vgg16` (4096 dim) as features of each video frame. This will result into a 4096x25 matrix for each video. \n",
        "    - Normalize your images using `torchvision.transforms` \n",
        "    ```\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    prep = transforms.Compose([ transforms.ToTensor(), normalize ])\n",
        "    prep(img)\n",
        "    The mean and std. mentioned above is specific to Imagenet data\n",
        "    \n",
        "    ```\n",
        "    More details of image preprocessing in PyTorch can be found at http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "    \n",
        "- **{35 pts} Modelling**. With the extracted features, build an LSTM network which takes a **dx25** sample as input (where **d** is the dimension of the extracted feature for each frame), and outputs the action label of that sample.\n",
        "- **{20 pts} Evaluation**. After training your network, you need to evaluate your model with the testing data by computing the prediction accuracy **(5 points)**. The baseline test accuracy for this data is 75%, and **10 points** out of 20 is for achieving test accuracy greater than the baseline. Moreover, you need to compare **(5 points)** the result of your network with that of support vector machine (SVM) (stacking the **dx25** feature matrix to a long vector and train a SVM).\n",
        "- **{10 pts} Report**. Details regarding the report can be found in the submission section below.\n",
        "\n",
        "Notice that the size of the raw images is 256x340, whereas your pre-trained model might take **nxn** images as inputs. To solve this problem, instead of resizing the images which unfavorably changes the spatial ratio, we take a better solution: Cropping five **nxn** images, one at the image center and four at the corners and compute the **d**-dim features for each of them, and average these five **d**-dim feature to get a final feature representation for the raw image.\n",
        "For example, VGG takes 224x224 images as inputs, so we take the five 224x224 croppings of the image, compute 4096-dim VGG features for each of them, and then take the mean of these five 4096-dim vectors to be the representation of the image.\n",
        "\n",
        "In order to save you computational time, you need to do the classification task only for **the first 25** classes of the whole dataset. The same applies to those who have access to GPUs. **Bonus 10 points for running and reporting on the entire 101 classes.**\n",
        "\n",
        "\n",
        "## Dataset\n",
        "Download **dataset** at [UCF101](http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_images.tar)(Image data for each video) and the **annos folder** which has the video labels and the label to class name mapping is included in the assignment folder uploaded. \n",
        "\n",
        "\n",
        "UCF101 dataset contains 101 actions and 13,320 videos in total.  \n",
        "\n",
        "+ `annos/actions.txt`  \n",
        "  + lists all the actions (`ApplyEyeMakeup`, .., `YoYo`)   \n",
        "  \n",
        "+ `annots/videos_labels_subsets.txt`  \n",
        "  + lists all the videos (`v_000001`, .., `v_013320`)  \n",
        "  + labels (`1`, .., `101`)  \n",
        "  + subsets (`1` for train, `2` for test)  \n",
        "\n",
        "+ `images/`  \n",
        "  + each folder represents a video\n",
        "  + the video/folder name to class mapping can be found using `annots/videos_labels_subsets.txt`, for e.g. `v_000001` belongs to class 1 i.e. `ApplyEyeMakeup`\n",
        "  + each video folder contains 25 frames  \n",
        "\n",
        "\n",
        "\n",
        "## Some Tutorials\n",
        "- Good materials for understanding RNN and LSTM\n",
        "    - http://blog.echen.me\n",
        "    - http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "    - http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "- Implementing RNN and LSTM with PyTorch\n",
        "    - [LSTM with PyTorch](http://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py)\n",
        "    - [RNN with PyTorch](http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UdCRwkL7jxtc"
      },
      "source": [
        "---\n",
        "---\n",
        "## **Problem 1.** Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "urKQi8oAjYb-",
        "colab": {}
      },
      "source": [
        "# \\*write your codes for feature extraction (You can use multiple cells, this is just a place holder)\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import torchvision.models as models\n",
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import pickle\n",
        "import cv2\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7zdxgWSmKvH",
        "colab_type": "code",
        "outputId": "73ee5675-30c2-4374-f680-6a2a4c8e27da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "vgg16 = models.vgg16(pretrained=True)\n",
        "vgg16.classifier = vgg16.classifier[:2]\n",
        "vgg16"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:21<00:00, 25.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cimdUJMbnSg9",
        "colab_type": "code",
        "outputId": "fa45f7e6-6234-40d5-8390-33e228874219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "videos_labels_subsets = open(\"annos/videos_labels_subsets.txt\", \"r\")\n",
        "dic = {}\n",
        "train_list = []\n",
        "test_list = []\n",
        "cnt=0\n",
        "for l in videos_labels_subsets:\n",
        "  lst = l[:-1].split('\\t')\n",
        "  if int(lst[1]) < 21 or int(lst[1]) > 25:\n",
        "    continue\n",
        "  dic['v_num'] = lst[0]\n",
        "  dic['class'] = lst[1]\n",
        "  if lst[2]=='1':\n",
        "    train_list.append(dic)\n",
        "  else:\n",
        "    test_list.append(dic)\n",
        "  dic = {}\n",
        "train_video_label_df = pd.DataFrame(train_list)\n",
        "test_video_label_df = pd.DataFrame(test_list)\n",
        "file = open('train_video_label_df.pkl','wb')\n",
        "pickle.dump(train_video_label_df, file)\n",
        "file.close()\n",
        "file = open('test_video_label_df.pkl','wb')\n",
        "pickle.dump(test_video_label_df, file)\n",
        "file.close()\n",
        "print(len(train_list))\n",
        "print(len(test_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "476\n",
            "190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjW3SdWR31dE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open('train_video_label_df.pkl', 'rb')\n",
        "train_video_label_df = pickle.load(file)\n",
        "file.close()\n",
        "file = open('test_video_label_df.pkl', 'rb')\n",
        "test_video_label_df = pickle.load(file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziKYGLam9wFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "videos_labels_subsets2 = open(\"annos/videos_labels_subsets.txt\", \"r\")\n",
        "img_count = 0\n",
        "for l in videos_labels_subsets2:\n",
        "  lst = l[:-1].split('\\t')\n",
        "  if int(lst[1]) > 25:\n",
        "    break\n",
        "  path, dirs, files = next(os.walk('/content/gdrive/My Drive/Y2019Fall/CSE-527-Intro-To-Computer-Vision/Paratkar_Shreyash_112673930_hw5/images/'+lst[0]))\n",
        "  img_count = img_count + len(files)\n",
        "img_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jn1lpFSBvss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_dir = '/content/gdrive/My Drive/Y2019Fall/CSE-527-Intro-To-Computer-Vision/Paratkar_Shreyash_112673930_hw5/images/'\n",
        "# image = cv2.imread(root_dir+'v_000001/i_0001.jpg')\n",
        "# # print(torch.tensor(image).shape)\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "prep = transforms.Compose([ transforms.ToTensor(), normalize ])\n",
        "# image = prep(image)\n",
        "# print(image.shape)\n",
        "# img_tp_lt = image[0:3, :224, :224]\n",
        "# print(img_tp_lt.shape)\n",
        "# img_bt_lt = image[0:3, 32:, :224]\n",
        "# print(img_bt_lt.shape)\n",
        "# img_bt_rt = image[0:3, 32:, 116:]\n",
        "# print(img_bt_rt.shape)\n",
        "# img_tp_rt = image[0:3, :224, 116:]\n",
        "# print(img_tp_rt.shape)\n",
        "# img_cn = image[0:3, 16:240, 58:282]\n",
        "# print(img_cn.shape)\n",
        "# labels = [1] * 125\n",
        "# labels = torch.Tensor(np.array(labels))\n",
        "# print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnEylWxov7rK",
        "colab_type": "code",
        "outputId": "774160c2-92d3-4ad2-8234-faa65930329c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.cuda.memory_cached()-torch.cuda.memory_allocated())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDHow1aV8SVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UCF101ImageDataset(Dataset):\n",
        "    \"\"\"UCF101 Actions dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, pickle_file, root_dir, transform=transforms.Compose([\n",
        "                                               transforms.ToTensor(),\n",
        "                                               transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                                           ])):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        file = open(pickle_file, 'rb')\n",
        "        self.actions_frame = pickle.load(file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.actions_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        action_num = self.actions_frame.iloc[idx, 1]\n",
        "        image_transforms = []\n",
        "        for i in range(1, 26):\n",
        "          img_name = ''\n",
        "          if i<10:\n",
        "            img_name = 'i_000'+str(i)+'.jpg'\n",
        "          else:\n",
        "            img_name = 'i_00'+str(i)+'.jpg'\n",
        "          parent_img_name = self.root_dir+self.actions_frame.iloc[idx, 0]+ '/'+ img_name\n",
        "          image = cv2.imread(parent_img_name)\n",
        "          # print(parent_img_name)\n",
        "          image = prep(image)\n",
        "          # print(image.shape)\n",
        "          image_transforms.append(image[0:3, :224, :224])\n",
        "          # print(img_tp_lt.shape)\n",
        "          image_transforms.append(image[0:3, 32:, :224])\n",
        "          # print(img_bt_lt.shape)\n",
        "          image_transforms.append(image[0:3, 32:, 116:])\n",
        "          # print(img_bt_rt.shape)\n",
        "          image_transforms.append(image[0:3, :224, 116:])\n",
        "          # print(img_tp_rt.shape)\n",
        "          image_transforms.append(image[0:3, 16:240, 58:282])\n",
        "          # print(img_cn.shape)\n",
        "        label = int(action_num)\n",
        "        # print(len(image_transforms))\n",
        "        # print(torch.stack(image_transforms).shape)\n",
        "        sample = {'images':torch.stack(image_transforms), 'action':label}\n",
        "        return sample\n",
        "train_dataset_func = UCF101ImageDataset(pickle_file='train_video_label_df.pkl', root_dir=root_dir)\n",
        "test_dataset_func = UCF101ImageDataset(pickle_file='test_video_label_df.pkl', root_dir=root_dir)\n",
        "train_dataloader = DataLoader(train_dataset_func, batch_size=1, shuffle=False, num_workers=4)\n",
        "test_dataloader = DataLoader(test_dataset_func, batch_size=1, shuffle=False, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ehVH66Hu3b5",
        "colab_type": "code",
        "outputId": "956dce66-dc3c-48c2-c65b-e727e69404b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Nov 22 07:59:51 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    26W / 149W |     11MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiukRyPChDnD",
        "colab_type": "code",
        "outputId": "755e24e9-fddb-49ea-ed7b-7cc4a0d0eadf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "model = vgg16\n",
        "model = model.cuda()\n",
        "count = 0\n",
        "train_features_data = []\n",
        "labels = []\n",
        "model.eval()\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "  for video_num, sample in enumerate(test_dataloader):\n",
        "    print(\"Video taken: \", video_num)\n",
        "    print(\"Time since start \", round(time.time() - start_time), \" seconds\")\n",
        "    video_images_tensor = sample['images'][0].cuda()\n",
        "    label = sample['action'][0].cuda()\n",
        "\n",
        "    vgg16output_125 = model(video_images_tensor)\n",
        "\n",
        "    del video_images_tensor\n",
        "\n",
        "    for i in range(0, 125, 5):\n",
        "      tensor = vgg16output_125[i]\n",
        "      # print(\"First tensor: \", tensor)\n",
        "      for j in range(1,5):\n",
        "        tensor+=vgg16output_125[i+j]\n",
        "      # print(\"Mean tensor: \", tensor)\n",
        "      train_features_data.append(tensor/5)\n",
        "      labels.append(label)\n",
        "      del tensor\n",
        "    del vgg16output_125, label\n",
        "print(\"Length of training features: \", len(train_features_data))\n",
        "print(\"Length labels: \", len(labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Video taken:  0\n",
            "Time since start  22  seconds\n",
            "Video taken:  1\n",
            "Time since start  24  seconds\n",
            "Video taken:  2\n",
            "Time since start  25  seconds\n",
            "Video taken:  3\n",
            "Time since start  26  seconds\n",
            "Video taken:  4\n",
            "Time since start  28  seconds\n",
            "Video taken:  5\n",
            "Time since start  29  seconds\n",
            "Video taken:  6\n",
            "Time since start  31  seconds\n",
            "Video taken:  7\n",
            "Time since start  32  seconds\n",
            "Video taken:  8\n",
            "Time since start  34  seconds\n",
            "Video taken:  9\n",
            "Time since start  36  seconds\n",
            "Video taken:  10\n",
            "Time since start  37  seconds\n",
            "Video taken:  11\n",
            "Time since start  38  seconds\n",
            "Video taken:  12\n",
            "Time since start  40  seconds\n",
            "Video taken:  13\n",
            "Time since start  41  seconds\n",
            "Video taken:  14\n",
            "Time since start  43  seconds\n",
            "Video taken:  15\n",
            "Time since start  44  seconds\n",
            "Video taken:  16\n",
            "Time since start  46  seconds\n",
            "Video taken:  17\n",
            "Time since start  47  seconds\n",
            "Video taken:  18\n",
            "Time since start  49  seconds\n",
            "Video taken:  19\n",
            "Time since start  50  seconds\n",
            "Video taken:  20\n",
            "Time since start  53  seconds\n",
            "Video taken:  21\n",
            "Time since start  54  seconds\n",
            "Video taken:  22\n",
            "Time since start  56  seconds\n",
            "Video taken:  23\n",
            "Time since start  57  seconds\n",
            "Video taken:  24\n",
            "Time since start  59  seconds\n",
            "Video taken:  25\n",
            "Time since start  61  seconds\n",
            "Video taken:  26\n",
            "Time since start  62  seconds\n",
            "Video taken:  27\n",
            "Time since start  64  seconds\n",
            "Video taken:  28\n",
            "Time since start  65  seconds\n",
            "Video taken:  29\n",
            "Time since start  67  seconds\n",
            "Video taken:  30\n",
            "Time since start  68  seconds\n",
            "Video taken:  31\n",
            "Time since start  70  seconds\n",
            "Video taken:  32\n",
            "Time since start  71  seconds\n",
            "Video taken:  33\n",
            "Time since start  73  seconds\n",
            "Video taken:  34\n",
            "Time since start  75  seconds\n",
            "Video taken:  35\n",
            "Time since start  76  seconds\n",
            "Video taken:  36\n",
            "Time since start  79  seconds\n",
            "Video taken:  37\n",
            "Time since start  80  seconds\n",
            "Video taken:  38\n",
            "Time since start  82  seconds\n",
            "Video taken:  39\n",
            "Time since start  83  seconds\n",
            "Video taken:  40\n",
            "Time since start  85  seconds\n",
            "Video taken:  41\n",
            "Time since start  87  seconds\n",
            "Video taken:  42\n",
            "Time since start  88  seconds\n",
            "Video taken:  43\n",
            "Time since start  89  seconds\n",
            "Video taken:  44\n",
            "Time since start  91  seconds\n",
            "Video taken:  45\n",
            "Time since start  95  seconds\n",
            "Video taken:  46\n",
            "Time since start  97  seconds\n",
            "Video taken:  47\n",
            "Time since start  98  seconds\n",
            "Video taken:  48\n",
            "Time since start  99  seconds\n",
            "Video taken:  49\n",
            "Time since start  101  seconds\n",
            "Video taken:  50\n",
            "Time since start  103  seconds\n",
            "Video taken:  51\n",
            "Time since start  104  seconds\n",
            "Video taken:  52\n",
            "Time since start  105  seconds\n",
            "Video taken:  53\n",
            "Time since start  108  seconds\n",
            "Video taken:  54\n",
            "Time since start  109  seconds\n",
            "Video taken:  55\n",
            "Time since start  110  seconds\n",
            "Video taken:  56\n",
            "Time since start  111  seconds\n",
            "Video taken:  57\n",
            "Time since start  113  seconds\n",
            "Video taken:  58\n",
            "Time since start  115  seconds\n",
            "Video taken:  59\n",
            "Time since start  116  seconds\n",
            "Video taken:  60\n",
            "Time since start  117  seconds\n",
            "Video taken:  61\n",
            "Time since start  120  seconds\n",
            "Video taken:  62\n",
            "Time since start  121  seconds\n",
            "Video taken:  63\n",
            "Time since start  122  seconds\n",
            "Video taken:  64\n",
            "Time since start  123  seconds\n",
            "Video taken:  65\n",
            "Time since start  126  seconds\n",
            "Video taken:  66\n",
            "Time since start  127  seconds\n",
            "Video taken:  67\n",
            "Time since start  128  seconds\n",
            "Video taken:  68\n",
            "Time since start  130  seconds\n",
            "Video taken:  69\n",
            "Time since start  132  seconds\n",
            "Video taken:  70\n",
            "Time since start  133  seconds\n",
            "Video taken:  71\n",
            "Time since start  135  seconds\n",
            "Video taken:  72\n",
            "Time since start  136  seconds\n",
            "Video taken:  73\n",
            "Time since start  138  seconds\n",
            "Video taken:  74\n",
            "Time since start  140  seconds\n",
            "Video taken:  75\n",
            "Time since start  141  seconds\n",
            "Video taken:  76\n",
            "Time since start  142  seconds\n",
            "Video taken:  77\n",
            "Time since start  145  seconds\n",
            "Video taken:  78\n",
            "Time since start  146  seconds\n",
            "Video taken:  79\n",
            "Time since start  147  seconds\n",
            "Video taken:  80\n",
            "Time since start  148  seconds\n",
            "Video taken:  81\n",
            "Time since start  152  seconds\n",
            "Video taken:  82\n",
            "Time since start  153  seconds\n",
            "Video taken:  83\n",
            "Time since start  154  seconds\n",
            "Video taken:  84\n",
            "Time since start  155  seconds\n",
            "Video taken:  85\n",
            "Time since start  158  seconds\n",
            "Video taken:  86\n",
            "Time since start  159  seconds\n",
            "Video taken:  87\n",
            "Time since start  160  seconds\n",
            "Video taken:  88\n",
            "Time since start  161  seconds\n",
            "Video taken:  89\n",
            "Time since start  164  seconds\n",
            "Video taken:  90\n",
            "Time since start  165  seconds\n",
            "Video taken:  91\n",
            "Time since start  166  seconds\n",
            "Video taken:  92\n",
            "Time since start  167  seconds\n",
            "Video taken:  93\n",
            "Time since start  169  seconds\n",
            "Video taken:  94\n",
            "Time since start  171  seconds\n",
            "Video taken:  95\n",
            "Time since start  172  seconds\n",
            "Video taken:  96\n",
            "Time since start  173  seconds\n",
            "Video taken:  97\n",
            "Time since start  175  seconds\n",
            "Video taken:  98\n",
            "Time since start  177  seconds\n",
            "Video taken:  99\n",
            "Time since start  178  seconds\n",
            "Video taken:  100\n",
            "Time since start  179  seconds\n",
            "Video taken:  101\n",
            "Time since start  182  seconds\n",
            "Video taken:  102\n",
            "Time since start  183  seconds\n",
            "Video taken:  103\n",
            "Time since start  184  seconds\n",
            "Video taken:  104\n",
            "Time since start  185  seconds\n",
            "Video taken:  105\n",
            "Time since start  188  seconds\n",
            "Video taken:  106\n",
            "Time since start  189  seconds\n",
            "Video taken:  107\n",
            "Time since start  190  seconds\n",
            "Video taken:  108\n",
            "Time since start  191  seconds\n",
            "Video taken:  109\n",
            "Time since start  194  seconds\n",
            "Video taken:  110\n",
            "Time since start  195  seconds\n",
            "Video taken:  111\n",
            "Time since start  196  seconds\n",
            "Video taken:  112\n",
            "Time since start  198  seconds\n",
            "Video taken:  113\n",
            "Time since start  201  seconds\n",
            "Video taken:  114\n",
            "Time since start  202  seconds\n",
            "Video taken:  115\n",
            "Time since start  203  seconds\n",
            "Video taken:  116\n",
            "Time since start  204  seconds\n",
            "Video taken:  117\n",
            "Time since start  207  seconds\n",
            "Video taken:  118\n",
            "Time since start  208  seconds\n",
            "Video taken:  119\n",
            "Time since start  209  seconds\n",
            "Video taken:  120\n",
            "Time since start  210  seconds\n",
            "Video taken:  121\n",
            "Time since start  213  seconds\n",
            "Video taken:  122\n",
            "Time since start  214  seconds\n",
            "Video taken:  123\n",
            "Time since start  215  seconds\n",
            "Video taken:  124\n",
            "Time since start  216  seconds\n",
            "Video taken:  125\n",
            "Time since start  219  seconds\n",
            "Video taken:  126\n",
            "Time since start  220  seconds\n",
            "Video taken:  127\n",
            "Time since start  221  seconds\n",
            "Video taken:  128\n",
            "Time since start  222  seconds\n",
            "Video taken:  129\n",
            "Time since start  224  seconds\n",
            "Video taken:  130\n",
            "Time since start  226  seconds\n",
            "Video taken:  131\n",
            "Time since start  227  seconds\n",
            "Video taken:  132\n",
            "Time since start  228  seconds\n",
            "Video taken:  133\n",
            "Time since start  230  seconds\n",
            "Video taken:  134\n",
            "Time since start  232  seconds\n",
            "Video taken:  135\n",
            "Time since start  233  seconds\n",
            "Video taken:  136\n",
            "Time since start  234  seconds\n",
            "Video taken:  137\n",
            "Time since start  237  seconds\n",
            "Video taken:  138\n",
            "Time since start  238  seconds\n",
            "Video taken:  139\n",
            "Time since start  239  seconds\n",
            "Video taken:  140\n",
            "Time since start  240  seconds\n",
            "Video taken:  141\n",
            "Time since start  243  seconds\n",
            "Video taken:  142\n",
            "Time since start  244  seconds\n",
            "Video taken:  143\n",
            "Time since start  245  seconds\n",
            "Video taken:  144\n",
            "Time since start  246  seconds\n",
            "Video taken:  145\n",
            "Time since start  250  seconds\n",
            "Video taken:  146\n",
            "Time since start  251  seconds\n",
            "Video taken:  147\n",
            "Time since start  252  seconds\n",
            "Video taken:  148\n",
            "Time since start  253  seconds\n",
            "Video taken:  149\n",
            "Time since start  256  seconds\n",
            "Video taken:  150\n",
            "Time since start  257  seconds\n",
            "Video taken:  151\n",
            "Time since start  258  seconds\n",
            "Video taken:  152\n",
            "Time since start  259  seconds\n",
            "Video taken:  153\n",
            "Time since start  263  seconds\n",
            "Video taken:  154\n",
            "Time since start  265  seconds\n",
            "Video taken:  155\n",
            "Time since start  266  seconds\n",
            "Video taken:  156\n",
            "Time since start  267  seconds\n",
            "Video taken:  157\n",
            "Time since start  270  seconds\n",
            "Video taken:  158\n",
            "Time since start  271  seconds\n",
            "Video taken:  159\n",
            "Time since start  272  seconds\n",
            "Video taken:  160\n",
            "Time since start  273  seconds\n",
            "Video taken:  161\n",
            "Time since start  276  seconds\n",
            "Video taken:  162\n",
            "Time since start  277  seconds\n",
            "Video taken:  163\n",
            "Time since start  279  seconds\n",
            "Video taken:  164\n",
            "Time since start  280  seconds\n",
            "Video taken:  165\n",
            "Time since start  285  seconds\n",
            "Video taken:  166\n",
            "Time since start  287  seconds\n",
            "Video taken:  167\n",
            "Time since start  288  seconds\n",
            "Video taken:  168\n",
            "Time since start  289  seconds\n",
            "Video taken:  169\n",
            "Time since start  292  seconds\n",
            "Video taken:  170\n",
            "Time since start  293  seconds\n",
            "Video taken:  171\n",
            "Time since start  294  seconds\n",
            "Video taken:  172\n",
            "Time since start  295  seconds\n",
            "Video taken:  173\n",
            "Time since start  298  seconds\n",
            "Video taken:  174\n",
            "Time since start  299  seconds\n",
            "Video taken:  175\n",
            "Time since start  301  seconds\n",
            "Video taken:  176\n",
            "Time since start  302  seconds\n",
            "Video taken:  177\n",
            "Time since start  305  seconds\n",
            "Video taken:  178\n",
            "Time since start  306  seconds\n",
            "Video taken:  179\n",
            "Time since start  307  seconds\n",
            "Video taken:  180\n",
            "Time since start  308  seconds\n",
            "Video taken:  181\n",
            "Time since start  312  seconds\n",
            "Video taken:  182\n",
            "Time since start  313  seconds\n",
            "Video taken:  183\n",
            "Time since start  314  seconds\n",
            "Video taken:  184\n",
            "Time since start  316  seconds\n",
            "Video taken:  185\n",
            "Time since start  319  seconds\n",
            "Video taken:  186\n",
            "Time since start  320  seconds\n",
            "Video taken:  187\n",
            "Time since start  321  seconds\n",
            "Video taken:  188\n",
            "Time since start  322  seconds\n",
            "Video taken:  189\n",
            "Time since start  326  seconds\n",
            "Length of training features:  4750\n",
            "Length labels:  4750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OeQ-XqiImIf",
        "colab_type": "code",
        "outputId": "16aafc7a-2876-45ea-8f50-8f9e6198c94d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# file = open('test_features_data_class_21_25.pkl','wb')\n",
        "# pickle.dump(train_features_data, file)\n",
        "# file.close()\n",
        "# file = open('test_labels_class_21_25.pkl','wb')\n",
        "# pickle.dump(labels, file)\n",
        "# file.close()\n",
        "print(\"Length of training features: \", len(train_features_data))\n",
        "print(\"Length labels: \", len(labels))\n",
        "# file = open('train_features_data_class_21_25.pkl','wb')\n",
        "# pickle.dump(train_features_data, file)\n",
        "# file.close()\n",
        "# file = open('labels_class_21_25.pkl','wb')\n",
        "# pickle.dump(labels, file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training features:  4750\n",
            "Length labels:  4750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpGLPCE7JJrr",
        "colab_type": "code",
        "outputId": "2976ac7f-b946-4001-ccdc-5474a4e0add3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# file = open('test_features_data_class_11_15.pkl', 'rb')\n",
        "# test_features_data_class_11_15 = pickle.load(file)\n",
        "# print(len(test_features_data_class_11_15))\n",
        "# file.close()\n",
        "# file = open('test_labels_class_11_15.pkl', 'rb')\n",
        "# test_labels_class_11_15 = pickle.load(file)\n",
        "# print(len(test_labels_class_11_15))\n",
        "# file.close()\n",
        "# file = open('train_features_data_class_1_5.pkl', 'rb')\n",
        "# train_features_data_class_0_5 = pickle.load(file)\n",
        "# print(len(train_features_data_class_0_5))\n",
        "# file.close()\n",
        "# file = open('train_features_data_class_1_5.pkl', 'rb')\n",
        "# labels_class_0_5 = pickle.load(file)\n",
        "# print(len(labels_class_0_5))\n",
        "# file.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11525\n",
            "11525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIHW6ruaN_Vj",
        "colab_type": "code",
        "outputId": "826160e1-f98b-4dbb-e424-22516d0755b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_pkls = ['train_features_data_class_1_5.pkl', 'train_features_data_class_6_10.pkl', 'train_features_data_class_11_15.pkl', 'train_features_data_class_16_20.pkl', 'train_features_data_class_21_25.pkl']\n",
        "train_label_pkls = ['labels_class_1_5.pkl', 'labels_class_6_10.pkl', 'labels_class_11_15.pkl', 'labels_class_16_20.pkl', 'labels_class_21_25.pkl']\n",
        "test_pkls = ['test_features_data_class_1_5.pkl', 'test_features_data_class_6_10.pkl',  'test_features_data_class_11_15.pkl', 'test_features_data_class_16_20.pkl', 'test_features_data_class_21_25.pkl']\n",
        "test_label_pkls = ['test_labels_class_1_5.pkl', 'test_labels_class_6_10.pkl', 'test_labels_class_11_15.pkl', 'test_labels_class_16_20.pkl', 'test_labels_class_21_25.pkl']\n",
        "train_list = []\n",
        "train_labels = []\n",
        "test_list = []\n",
        "test_labels = []\n",
        "for i in range(0,5):\n",
        "  train_list.extend(pickle.load(open(train_pkls[i], 'rb')))\n",
        "  train_labels.extend(pickle.load(open(train_label_pkls[i], 'rb')))\n",
        "  test_list.extend(pickle.load(open(test_pkls[i], 'rb')))\n",
        "  test_labels.extend(pickle.load(open(test_label_pkls[i], 'rb')))\n",
        "print(len(train_list))\n",
        "print(len(train_labels))\n",
        "print(len(test_list))\n",
        "print(len(test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60225\n",
            "60225\n",
            "23775\n",
            "23775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bA3mfAXLSnD",
        "colab_type": "code",
        "outputId": "7713a146-ab8f-4460-b522-127d64f9662f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_data_final = torch.stack(train_list)\n",
        "print(\"Training data size:\\t\", train_data_final.size(), \"\\t type: \", type(train_data_final), \"\\t\\t element: \", (train_data_final[0]))\n",
        "train_labels_final = torch.stack(train_labels)\n",
        "print(\"Training labels size:\\t\", train_labels_final.size(), \"\\t\\t type: \", type(train_labels_final), \"\\t\\t element: \", (train_labels_final[0]))\n",
        "test_data_final = torch.stack(test_list)\n",
        "print(\"Testing data size:\\t\", test_data_final.size(), \"\\t type: \", type(test_data_final), \"\\t\\t element: \", (test_data_final[0]))\n",
        "test_labels_final = torch.stack(test_labels)\n",
        "print(\"Testing labels size:\\t\", test_labels_final.size(), \"\\t\\t type: \", type(test_labels_final), \"\\t\\t element: \", (test_labels_final[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data size:\t torch.Size([60225, 4096]) \t type:  <class 'torch.Tensor'> \t\t element:  tensor([0.0542, 1.0759, 0.5138,  ..., 0.0000, 0.3470, 3.8192], device='cuda:0')\n",
            "Training labels size:\t torch.Size([60225]) \t\t type:  <class 'torch.Tensor'> \t\t element:  tensor(1, device='cuda:0')\n",
            "Testing data size:\t torch.Size([23775, 4096]) \t type:  <class 'torch.Tensor'> \t\t element:  tensor([0.0694, 0.0000, 0.0000,  ..., 0.0000, 1.7150, 1.0321], device='cuda:0')\n",
            "Testing labels size:\t torch.Size([23775]) \t\t type:  <class 'torch.Tensor'> \t\t element:  tensor(1, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rdjb7ih83tOY"
      },
      "source": [
        "***\n",
        "***\n",
        "## **Problem 2.** Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Do5OSV9Mnmwy"
      },
      "source": [
        "* ##### **Print the size of your training and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0EGU31IJn5_h",
        "outputId": "8ab1b5cb-c828-429d-b69f-fa6d5450b066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Don't hardcode the shape of train and test data\n",
        "print('Shape of training data is :', )\n",
        "print(\"Training data size:\\t\", train_data_final.size(), \"\\t shape: \", train_data_final.shape, \"\\t type: \", type(train_data_final), \"\\t element: \", (train_data_final[0]))\n",
        "print(\"Training labels size:\\t\", train_labels_final.size(), \"\\t\\t shape: \", train_labels_final.shape, \"\\t\\t type: \", type(train_labels_final), \"\\t element: \", (train_labels_final[0]))\n",
        "print('Shape of test/validation data is :', )\n",
        "print(\"Testing data size:\\t\", test_data_final.size(), \"\\t shape: \", test_data_final.shape, \"\\t type: \", type(test_data_final), \"\\t element: \", (test_data_final[0]))\n",
        "print(\"Testing labels size:\\t\", test_labels_final.size(), \"\\t\\t shape: \", test_labels_final.shape, \"\\t\\t type: \", type(test_labels_final), \"\\t element: \", (test_labels_final[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training data is :\n",
            "Training data size:\t torch.Size([60225, 4096]) \t shape:  torch.Size([60225, 4096]) \t type:  <class 'torch.Tensor'> \t element:  tensor([0.0542, 1.0759, 0.5138,  ..., 0.0000, 0.3470, 3.8192], device='cuda:0')\n",
            "Training labels size:\t torch.Size([60225]) \t\t shape:  torch.Size([60225]) \t\t type:  <class 'torch.Tensor'> \t element:  tensor(1, device='cuda:0')\n",
            "Shape of test/validation data is :\n",
            "Testing data size:\t torch.Size([23775, 4096]) \t shape:  torch.Size([23775, 4096]) \t type:  <class 'torch.Tensor'> \t element:  tensor([0.0694, 0.0000, 0.0000,  ..., 0.0000, 1.7150, 1.0321], device='cuda:0')\n",
            "Testing labels size:\t torch.Size([23775]) \t\t shape:  torch.Size([23775]) \t\t type:  <class 'torch.Tensor'> \t element:  tensor(1, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SKttbANQc41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open('train_data_final.pkl','wb')\n",
        "pickle.dump(train_data_final, file)\n",
        "file.close()\n",
        "file = open('train_labels_final.pkl','wb')\n",
        "pickle.dump(train_labels_final, file)\n",
        "file.close()\n",
        "file = open('test_data_final.pkl','wb')\n",
        "pickle.dump(test_data_final, file)\n",
        "file.close()\n",
        "file = open('test_labels_final.pkl','wb')\n",
        "pickle.dump(test_labels_final, file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-CYCmhw2LC5",
        "colab_type": "text"
      },
      "source": [
        "# Load final data from here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibbVnLuqQ4pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_final = pickle.load(open('train_data_final.pkl', 'rb'))\n",
        "train_labels_final = pickle.load(open('train_labels_final.pkl', 'rb'))\n",
        "test_data_final = pickle.load(open('test_data_final.pkl', 'rb'))\n",
        "test_labels_final = pickle.load(open('test_labels_final.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q12qVFPRHNG",
        "colab_type": "code",
        "outputId": "b9efd984-cde6-4bf9-e0b5-7ca47d6a9913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Training data size:\\t\", train_data_final.size(), \"\\t shape: \", train_data_final.shape, \"\\t type: \", type(train_data_final), \"\\t element: \", (train_data_final[0]))\n",
        "print(\"Training labels size:\\t\", train_labels_final.size(), \"\\t\\t shape: \", train_labels_final.shape, \"\\t\\t type: \", type(train_labels_final), \"\\t element: \", (train_labels_final[0]))\n",
        "print(\"Testing data size:\\t\", test_data_final.size(), \"\\t shape: \", test_data_final.shape, \"\\t type: \", type(test_data_final), \"\\t element: \", (test_data_final[0]))\n",
        "print(\"Testing labels size:\\t\", test_labels_final.size(), \"\\t\\t shape: \", test_labels_final.shape, \"\\t\\t type: \", type(test_labels_final), \"\\t element: \", (test_labels_final[0]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data size:\t torch.Size([60225, 4096]) \t shape:  torch.Size([60225, 4096]) \t type:  <class 'torch.Tensor'> \t element:  tensor([0.0542, 1.0759, 0.5138,  ..., 0.0000, 0.3470, 3.8192], device='cuda:0')\n",
            "Training labels size:\t torch.Size([60225]) \t\t shape:  torch.Size([60225]) \t\t type:  <class 'torch.Tensor'> \t element:  tensor(1, device='cuda:0')\n",
            "Testing data size:\t torch.Size([23775, 4096]) \t shape:  torch.Size([23775, 4096]) \t type:  <class 'torch.Tensor'> \t element:  tensor([0.0694, 0.0000, 0.0000,  ..., 0.0000, 1.7150, 1.0321], device='cuda:0')\n",
            "Testing labels size:\t torch.Size([23775]) \t\t shape:  torch.Size([23775]) \t\t type:  <class 'torch.Tensor'> \t element:  tensor(1, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BHG9JB3ZBTx",
        "colab_type": "code",
        "outputId": "2b41c4b4-72a6-41f3-9898-d4a344a7da51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_data_final_split = torch.stack(torch.split(train_data_final, 25))\n",
        "print(train_data_final_split.shape)\n",
        "train_labels_final_split = torch.stack(torch.split(train_labels_final, 25))\n",
        "train_lab = []\n",
        "for i in range(train_labels_final_split.shape[0]):\n",
        "  # print(type(torch.max(torch.stack(torch.split(train_labels_final_split[i], 1)))))\n",
        "  train_lab.append(torch.max(torch.stack(torch.split(train_labels_final_split[i], 1))))\n",
        "train_labels_final_split = torch.tensor(train_lab)\n",
        "print(train_labels_final_split.shape)\n",
        "test_data_final_split = torch.stack(torch.split(test_data_final, 25))\n",
        "print(test_data_final_split.shape)\n",
        "test_labels_final_split = torch.stack(torch.split(test_labels_final, 25))\n",
        "test_lab = []\n",
        "for i in range(test_labels_final_split.shape[0]):\n",
        "  # print(type(torch.max(torch.stack(torch.split(train_labels_final_split[i], 1)))))\n",
        "  test_lab.append(torch.max(torch.stack(torch.split(test_labels_final_split[i], 1))))\n",
        "test_labels_final_split = torch.tensor(test_lab)\n",
        "print(test_labels_final_split.shape)\n",
        "del train_lab, train_data_final, train_labels_final, test_lab, test_data_final, test_labels_final"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2409, 25, 4096])\n",
            "torch.Size([2409])\n",
            "torch.Size([951, 25, 4096])\n",
            "torch.Size([951])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uqshyjO3mHkt",
        "outputId": "7513f8c0-ac43-41dd-dfb4-07574d204058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import random\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1, num_layers=2):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
        "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim), torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
        "\n",
        "    def forward(self, input):\n",
        "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
        "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
        "        return y_pred.view(-1)\n",
        "\n",
        "def trainLSTM(model, video_features, video_labels, epochs, learning_rate,optimizer=None):\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  model = model.cuda(device)\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  start_time = time.time()\n",
        "  for ep in range(epochs):\n",
        "      data = list(zip(video_features, video_labels))\n",
        "      random.shuffle(data)\n",
        "      video_features, video_labels = zip(*data)\n",
        "      correct_count = 0\n",
        "      total_count = 0\n",
        "      for i in range(len(video_features)):\n",
        "          video_feature = video_features[i]\n",
        "          label_feature = video_labels[i].type(torch.LongTensor)\n",
        "          label_feature = label_feature.view(1) - 1\n",
        "          model.zero_grad()        \n",
        "          model.hidden = model.init_hidden()\n",
        "          preds = model(video_feature.cuda(device))\n",
        "          preds = preds.view(1, preds.shape[0])\n",
        "          loss = loss_func(preds, label_feature.cuda(device)) \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          pred_indices, pred_vals = torch.max(preds.data, 1)\n",
        "          total_count = total_count + label_feature.size(0)\n",
        "          correct_count = correct_count + (pred_vals == label_feature.cuda(device)).sum().item()\n",
        "      torch.cuda.empty_cache()\n",
        "      print('Epoch: ', ep + 1, ' = ' , round((correct_count / total_count) * 100, 3), '%')\n",
        "\n",
        "  print('\\nTraining time = ', round(time.time() - start_time), ' seconds')\n",
        "  return (correct_count / total_count) * 100\n",
        "model = LSTM(input_dim=4096, hidden_dim=1024, num_layers=2, batch_size=1, output_dim = 25)\n",
        "print('Training accuracies per epoch:\\n')\n",
        "train_accuracy = trainLSTM(model, train_data_final_split, train_labels_final_split, 5, 0.00001)\n",
        "print('LSTM Training Accuracy = ', train_accuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracies per epoch:\n",
            "\n",
            "Epoch:  1  =  74.72 %\n",
            "Epoch:  2  =  99.045 %\n",
            "Epoch:  3  =  99.751 %\n",
            "Epoch:  4  =  99.917 %\n",
            "Epoch:  5  =  100.0 %\n",
            "\n",
            "Training time =  228  seconds\n",
            "LSTM Training Accuracy =  100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm-AqFKITY5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e53e8a35-b626-4577-ef4e-ca46de84dc7a"
      },
      "source": [
        "import time\n",
        "def test_model(model, video_features, video_labels):\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  start_time = time.time()\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      correct_count = 0\n",
        "      total_count = 0\n",
        "      for i in range(video_features.shape[0]):\n",
        "          video_feature = video_features[i]\n",
        "          label_feature = video_labels[i].type(torch.LongTensor)\n",
        "          label_feature = label_feature.view(1) - 1\n",
        "\n",
        "          preds = model(video_feature.cuda(device))\n",
        "          preds = preds.view(1, preds.shape[0])\n",
        "          _, pred_vals = torch.max(preds.data, 1)\n",
        "          total_count = total_count + label_feature.size(0)\n",
        "          correct_count = correct_count + (pred_vals == label_feature.cuda(device)).sum().item()\n",
        "\n",
        "      print('Test accuracy = ' , round((correct_count / total_count) * 100, 3), '%')\n",
        "  \n",
        "  print('Testing time: ', round(time.time() - start_time), ' seconds')\n",
        "  return (correct_count / total_count) * 100\n",
        "test_accuracy = test_model(model, test_data_final_split, test_labels_final_split)\n",
        "print('LSTM Testing Accuracy = ', test_accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy =  80.967 %\n",
            "Testing time:  4  seconds\n",
            "LSTM Testing Accuracy =  80.96740273396425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IduZuc2GgoS",
        "colab_type": "text"
      },
      "source": [
        "Reference:\n",
        "\n",
        "https://www.jessicayung.com/lstms-for-time-series-in-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2x5vDxCw_JF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_for_linearSVC = train_data_final_split.view(train_data_final_split.shape[0], train_data_final_split.shape[1]* train_data_final_split.shape[2]).cpu().detach().numpy()\n",
        "train_labels_for_linearSVC = train_labels_final_split.cpu().detach().numpy()\n",
        "test_for_linearSVC = test_data_final_split.view(test_data_final_split.shape[0], test_data_final_split.shape[1]* test_data_final_split.shape[2]).cpu().detach().numpy()\n",
        "test_labels_for_linearSVC = test_labels_final_split.cpu().detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uU5HG2Ufm0-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d501a3b7-a275-45be-a292-599a402fd9f6"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "start_time = time.time()\n",
        "linearSVCClassifier = LinearSVC(C=0.0001, multi_class=\"ovr\")\n",
        "linearSVCClassifier.fit(train_for_linearSVC, train_labels_for_linearSVC)\n",
        "pred = linearSVCClassifier.predict(test_for_linearSVC)\n",
        "test_accuracy_SVC = accuracy_score(pred, test_labels_for_linearSVC)*100\n",
        "print('Test accuracy: ', test_accuracy_SVC, '%')\n",
        "print('Testing time: ', round(time.time() - start_time), ' seconds')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:  86.01472134595163 %\n",
            "Testing time:  195  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_JYQYjazVV3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2035f445-400d-40c3-8292-d46ed7ec00d3"
      },
      "source": [
        "linearSVCClassifier = LinearSVC(C=0.0001, multi_class=\"ovr\")\n",
        "linearSVCClassifier.fit(train_for_linearSVC, train_labels_for_linearSVC)\n",
        "pred = linearSVCClassifier.predict(train_for_linearSVC)\n",
        "train_accuracy_SVC = accuracy_score(pred, train_labels_for_linearSVC)*100\n",
        "print('Training accuracy: ', train_accuracy_SVC, '%')\n",
        "print('Training time: ', round(time.time() - start_time), ' seconds')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy:  100.0 %\n",
            "Training time:  388  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v0znm2TMmsDZ"
      },
      "source": [
        "---\n",
        "---\n",
        "## **Problem 3.** Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ARtMhcbXmsXk",
        "colab": {}
      },
      "source": [
        "# \\*write your codes for evaluation (You can use multiple cells, this is just a place holder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "80ZUeqnGv48f"
      },
      "source": [
        "* ##### **Print the train and test accuracy of your model** **- Printed Above**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6c0f0875-605d-48e8-9b06-337a20cdc691",
        "id": "mlUW9cly5OxR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Don't hardcode the train and test accuracy\n",
        "print('Training accuracy for LSTM is %2.3f :' %(train_accuracy) )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy for LSTM is 100.000 :\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dm24kNE6iCa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "baae8865-c496-4742-ca69-41df69e89fee"
      },
      "source": [
        "print('Test accuracy for LSTM is %2.3f :' %(test_accuracy) )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy for LSTM is 80.967 :\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eesNQn6FYKQz"
      },
      "source": [
        "* ##### **Print the train and test and test accuracy of SVM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ip87hPqTYJtr",
        "outputId": "106d7cf7-e7cc-4b75-ed85-64428229f6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Don't hardcode the train and test accuracy\n",
        "print('Training accuracy for SVC is %2.3f' %(train_accuracy_SVC))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy for SVC is 100.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2DHpCmN6_w_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82c021af-dc3f-44bb-c113-c23d11d398e1"
      },
      "source": [
        "print('Test accuracy for SVC is %2.3f' %(test_accuracy_SVC))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy for SVC is 86.015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cL4Y3nHBkwmb"
      },
      "source": [
        "## **Problem 4.** Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbzpDeRRNuHj",
        "colab_type": "text"
      },
      "source": [
        "## **Bonus**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jPrhLzuyN-rr"
      },
      "source": [
        "* ##### **Print the size of your training and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8IR6zrwOENz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't hardcode the shape of train and test data\n",
        "print('Shape of training data is :', )\n",
        "print('Shape of test/validation data is :', )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GAakVg8-OE_j"
      },
      "source": [
        "* ##### **Modelling and evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHbPzkcoObLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write your code for modelling and evaluation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vb4Wlzw2jYcJ"
      },
      "source": [
        "## Submission\n",
        "---\n",
        "**Runnable source code in ipynb file and a pdf report are required**.\n",
        "\n",
        "The report should be of 3 to 4 pages describing what you have done and learned in this homework and report performance of your model. If you have tried multiple methods, please compare your results. If you are using any external code, please cite it in your report. Note that this homework is designed to help you explore and get familiar with the techniques. The final grading will be largely based on your prediction accuracy and the different methods you tried (different architectures and parameters).\n",
        "\n",
        "Please indicate clearly in your report what model you have tried, what techniques you applied to improve the performance and report their accuracies. The report should be concise and include the highlights of your efforts.\n",
        "The naming convention for report is **Surname_Givenname_SBUID_report*.pdf**\n",
        "\n",
        "When submitting your .zip file through blackboard, please\n",
        "-- name your .zip file as **Surname_Givenname_SBUID_hw*.zip**.\n",
        "\n",
        "This zip file should include:\n",
        "```\n",
        "Surname_Givenname_SBUID_hw*\n",
        "        |---Surname_Givenname_SBUID_hw*.ipynb\n",
        "        |---Surname_Givenname_SBUID_hw*.pdf\n",
        "        |---Surname_Givenname_SBUID_report*.pdf\n",
        "```\n",
        "\n",
        "For instance, student Michael Jordan should submit a zip file named \"Jordan_Michael_111134567_hw5.zip\" for homework5 in this structure:\n",
        "```\n",
        "Jordan_Michael_111134567_hw5\n",
        "        |---Jordan_Michael_111134567_hw5.ipynb\n",
        "        |---Jordan_Michael_111134567_hw5.pdf\n",
        "        |---Jordan_Michael_111134567_report*.pdf\n",
        "```\n",
        "\n",
        "The **Surname_Givenname_SBUID_hw*.pdf** should include a **google shared link**. To generate the **google shared link**, first create a folder named **Surname_Givenname_SBUID_hw*** in your Google Drive with your Stony Brook account. \n",
        "\n",
        "Then right click this folder, click ***Get shareable link***, in the People textfield, enter two TA's emails: ***bo.cao.1@stonybrook.edu*** and ***sayontan.ghosh@stonybrook.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n",
        "\n",
        "Colab has a good feature of version control, you should take advantage of this to save your work properly. However, the timestamp of the submission made in blackboard is the only one that we consider for grading. To be more specific, we will only grade the version of your code right before the timestamp of the submission made in blackboard. \n",
        "\n",
        "You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be dealys in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n",
        "\n",
        "Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwEVeiU4hqxz",
        "colab_type": "text"
      },
      "source": [
        "Drive Link: https://drive.google.com/drive/folders/1Wi5pmgQ_hBVxGJOBYf943Q5kc01ff6fI?usp=sharing"
      ]
    }
  ]
}